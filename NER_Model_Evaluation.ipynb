{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "KFgkxM2b8WRr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "tag_names = [\"Sign_symptom\" , \"Diagnostic_procedure\", \"Biological_structure\", \"Disease_disorder\",\"Medication\"]\n",
        "# TODO:Change these paths\n",
        "parent_path = \"/MACCROBAT2018/\"\n",
        "ner_model_path =\"/ner_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzAmkmvgID2D"
      },
      "source": [
        "# Get Annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "H_g3WLe-4Hhd"
      },
      "outputs": [],
      "source": [
        "# get all annotations and text file\n",
        "\n",
        "def get_annotations(ann_file_path : str , txt_file_path:str, tag_names:list):\n",
        "  \"\"\"\n",
        "  return format : [text_file_content, (entity_label, start, end, term), ... (entity_label, start, end, term)]\n",
        "  \"\"\"\n",
        "  # Get all file contents\n",
        "  ann_fhand = open(ann_file_path, \"r\")\n",
        "  text_fhand = open(txt_file_path, \"r\")\n",
        "\n",
        "  ann_file_content = ann_fhand.read()\n",
        "  text_file_content = text_fhand.read();\n",
        "\n",
        "  ann_fhand.close()\n",
        "  text_fhand.close()\n",
        "\n",
        "  annotation_group = [text_file_content]\n",
        "\n",
        "  # Extracting the lines that include desired labels/enities\n",
        "  regex = r'^T.*(?:' + '|'.join(tag_names) + r').*$'\n",
        "  all_valid = re.findall(regex , ann_file_content , flags = re.MULTILINE)\n",
        "\n",
        "  # Processing into Annottaions\n",
        "  for valid_line in all_valid:\n",
        "    line_components = valid_line.split()\n",
        "    entity_type,start_index, end_index , *term = line_components[1:]\n",
        "    entity_type = entity_type.upper()\n",
        "    term = \" \".join(term)\n",
        "    if(not(start_index.isdigit() and end_index.isdigit())):\n",
        "      continue\n",
        "    start_index = int(start_index)\n",
        "    end_index = int(end_index)\n",
        "    entity_type = entity_type.strip()\n",
        "    annotation = (entity_type, start_index,end_index, term)\n",
        "\n",
        "    # Add formed annotation to annotation group\n",
        "    annotation_group.append(annotation)\n",
        "\n",
        "  return annotation_group\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB7r6NutILIs"
      },
      "source": [
        "# Transformation of annotations to Dictionary of annotation groups with frequency of words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "BFsXk6n568-n"
      },
      "outputs": [],
      "source": [
        "# transform to dict format for entites\n",
        "def transform_to_dict_format(annotation_group):\n",
        "  entities = annotation_group[1:]\n",
        "\n",
        "  main_dict = {}\n",
        "  for entity in entities:\n",
        "    entity_type, start_index, end_index, term = entity\n",
        "    if(entity_type not in main_dict):\n",
        "      main_dict[entity_type] = {};\n",
        "    main_dict[entity_type][term] = main_dict[entity_type].get(term,0) +1\n",
        "  return [annotation_group[0], main_dict]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWskLM5jM6jJ"
      },
      "source": [
        "# Functions and Evaluation metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "gWMzUC2C-Hex"
      },
      "outputs": [],
      "source": [
        "my_model = spacy.load(ner_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "KhHSXrNA-ePL"
      },
      "outputs": [],
      "source": [
        "def evaluate_ner(filepath):\n",
        "\n",
        "  total_entities = 0\n",
        "  found_entities =0\n",
        "  correct_entities = 0\n",
        "  incorrect_entities = 0\n",
        "  incorrect_dict = dict()\n",
        "  txt_file= filepath+\".txt\"\n",
        "  ann_file = filepath + \".ann\"\n",
        "\n",
        "  ann_group = get_annotations(ann_file_path=ann_file, txt_file_path=txt_file, tag_names=tag_names)\n",
        "  ann_dict = transform_to_dict_format(ann_group)\n",
        "\n",
        "  content = ann_dict[0]\n",
        "\n",
        "  entities_dict = ann_dict[1]\n",
        "\n",
        "  count = 0;\n",
        "  for i in entities_dict:\n",
        "    for j in (list(entities_dict[i].keys())):\n",
        "      count += (entities_dict[i][j])\n",
        "\n",
        "  total_entities += count\n",
        "\n",
        "  # run ner model on text\n",
        "  doc = my_model(content)\n",
        "  for ent in doc.ents:\n",
        "    # print(ent.text, ent.label_)\n",
        "\n",
        "    if(ent.label_ in entities_dict):\n",
        "      # declare entity's existance\n",
        "      found_entities += 1\n",
        "\n",
        "      if(ent.text in entities_dict[ent.label_] and entities_dict[ent.label_][ent.text] > 0):\n",
        "        # declare correct entity label\n",
        "        correct_entities += 1\n",
        "        # remove term count\n",
        "        entities_dict[ent.label_][ent.text] -= 1\n",
        "      else:\n",
        "        # declare incorrect entity label\n",
        "        incorrect_dict[ent.label_] = incorrect_dict.get(ent.label_,0) + 1\n",
        "        incorrect_entities += 1\n",
        "  # print(entities_dict)\n",
        "  unidentified_entities = 0\n",
        "  unidentified_dict = dict()\n",
        "  for i in entities_dict:\n",
        "    for j in (list(entities_dict[i].keys())):\n",
        "      if(entities_dict[i][j] > 0):\n",
        "        unidentified_entities += (entities_dict[i][j])\n",
        "        unidentified_dict[i] = unidentified_dict.get(i,0) + entities_dict[i][j]\n",
        "  # print(unidentified_dict)\n",
        "  return (total_entities, found_entities, correct_entities, incorrect_entities, unidentified_entities, incorrect_dict, unidentified_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ndyquRp7GdPq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def evaluate_all(path):\n",
        "  fhand_filenames = open(parent_path +\"all_files.txt\",\"r\") # This file contains all the names of the ann/txt file\n",
        "  all_filenames = fhand_filenames.readlines()\n",
        "  fhand_filenames.close()\n",
        "\n",
        "  all_total_entities = 0\n",
        "  all_found_entities =0\n",
        "  all_correct_entities = 0\n",
        "  all_incorrect_entities = 0\n",
        "  all_unidentified_entities = 0\n",
        "  all_incorrect_dict = dict()\n",
        "  all_unidentified_dict = dict()\n",
        "\n",
        "  basenames = [filename[:-5] for filename in all_filenames if filename.endswith(\".txt\\n\")]\n",
        "\n",
        "  for basename in tqdm(basenames):\n",
        "    if basename == \"all_files\":\n",
        "      continue\n",
        "    total_entities, found_entities, correct_entities, incorrect_entities,unidentified_entities, incorrect_dict ,unidentified_dict= evaluate_ner(parent_path + basename)\n",
        "    all_total_entities += total_entities\n",
        "    all_found_entities += found_entities\n",
        "    all_correct_entities += correct_entities\n",
        "    all_incorrect_entities += incorrect_entities\n",
        "    all_unidentified_entities += unidentified_entities\n",
        "    for i in incorrect_dict:\n",
        "      all_incorrect_dict[i] = all_incorrect_dict.get(i,0) + incorrect_dict[i]\n",
        "    for i in unidentified_dict:\n",
        "      all_unidentified_dict[i] = all_unidentified_dict.get(i,0) + unidentified_dict[i]\n",
        "\n",
        "  print(\"\\nTotal entities: \", all_total_entities)\n",
        "  print(\"Found entities: \", all_found_entities, \" \" , (all_found_entities/all_total_entities)*100)\n",
        "  print(\"Correct entities: \", all_correct_entities, \" \" , (all_correct_entities/all_found_entities)*100)\n",
        "  print(\"Incorrect entities: \", all_incorrect_entities , \" \",  (all_incorrect_entities/all_found_entities)*100)\n",
        "  print(\"Unidentified entities: \", all_unidentified_entities , \" \",  (all_unidentified_entities/all_total_entities)*100)\n",
        "  print(\"Incorrect Dict : \", str(all_incorrect_dict))\n",
        "  print(\"Unidentified Dict : \", str(all_unidentified_dict))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P52IA_emHxp-",
        "outputId": "58e9c1a8-db12-42eb-8cf8-b05548858af9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 201/201 [00:09<00:00, 22.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total entities:  13261\n",
            "Found entities:  13288   100.20360455470929\n",
            "Correct entities:  13063   98.30674292594821\n",
            "Incorrect entities:  225   1.6932570740517758\n",
            "Unidentified entities:  198   1.493100067868185\n",
            "Incorrect Dict :  {'DIAGNOSTIC_PROCEDURE': 79, 'SIGN_SYMPTOM': 53, 'DISEASE_DISORDER': 23, 'BIOLOGICAL_STRUCTURE': 57, 'MEDICATION': 13}\n",
            "Unidentified Dict :  {'BIOLOGICAL_STRUCTURE': 44, 'DISEASE_DISORDER': 31, 'DIAGNOSTIC_PROCEDURE': 71, 'SIGN_SYMPTOM': 39, 'MEDICATION': 13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_all(parent_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kzAmkmvgID2D",
        "HB7r6NutILIs",
        "XWskLM5jM6jJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
