{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7MDx6VVCulA"
      },
      "source": [
        "# Code for Extractive Summarization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYAN_4_d91yb",
        "outputId": "4d398600-8997-4fad-aabc-62126464cc22"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import os, csv\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# from tabulate import tabulate\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "# CHANGE THIS\n",
        "base_path = \"TestSummarizer\"\n",
        "folder_path = \"TestSummarizer/business_summary\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DJOGpb5q7S3f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def find_similarity(sentence1:list, sentence2:list, stopwords=None):\n",
        "\n",
        "  # Removing all stop words\n",
        "  filtered_sentence1 = [word.lower() for word in sentence1 if word not in stopwords]\n",
        "  filtered_sentence2 = [word.lower() for word in sentence2 if word not in stopwords]\n",
        "\n",
        "\n",
        "  # get unique words\n",
        "  unique_words = list()\n",
        "  unique_words.extend(filtered_sentence1)\n",
        "  unique_words.extend(filtered_sentence2)\n",
        "\n",
        "  unique_words = list(set(unique_words)) # use set to ensure only one case of each word\n",
        "\n",
        "  # Get frequency information\n",
        "  frequency_1 = dict()\n",
        "  frequency_2 = dict()\n",
        "\n",
        "  for word in filtered_sentence1:\n",
        "    frequency_1[word]  = frequency_1.get(word, 0) +1;\n",
        "\n",
        "  for word in filtered_sentence2:\n",
        "    frequency_2[word]  = frequency_2.get(word, 0) +1;\n",
        "\n",
        "  # Create vectors\n",
        "  size_unique_words = len(unique_words)\n",
        "  vector_sentence1 = [0 for i in unique_words]\n",
        "  vector_sentence2 = [0 for i in unique_words]\n",
        "\n",
        "  # Assigning frequencies to vectors\n",
        "  for word,frequency in frequency_1.items():\n",
        "    vector_sentence1[unique_words.index(word)] = frequency\n",
        "\n",
        "  for word,frequency in frequency_2.items():\n",
        "    vector_sentence2[unique_words.index(word)] =frequency\n",
        "  # Calculate similarity between vectors\n",
        "  return 1-cosine_distance(vector_sentence1, vector_sentence2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nOmxs0lQ7UlB"
      },
      "outputs": [],
      "source": [
        "def construct_similarity_matrix(all_sentences, stopwords):\n",
        "  # Initialize matrix\n",
        "  matrix = [\n",
        "\n",
        "            [0 for i in range(len(all_sentences))]\n",
        "            for j in range(len(all_sentences))\n",
        "  ]\n",
        "  # Iterate over all sentences\n",
        "  for i in range(len(all_sentences)):\n",
        "    for j in range(len(all_sentences)):\n",
        "      # Calculate similarity score for each sentence pair\n",
        "      if(i != j):\n",
        "        matrix[i][j] = find_similarity(all_sentences[i], all_sentences[j], stopwords)\n",
        "      else:\n",
        "        # Skip same sentence pairs.\n",
        "        continue\n",
        "  return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-R5u11c47WU3"
      },
      "outputs": [],
      "source": [
        "def text_rank(similarity_matrix):\n",
        "    # Initialize scores\n",
        "    sentence_count = len(similarity_matrix)\n",
        "    scores = [ 1 for i in range(sentence_count)]\n",
        "    # Set damping factors\n",
        "    damping_factor = 0.85\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    for x in range(100):\n",
        "        # Initialize prior scores\n",
        "        prior_scores = scores.copy()\n",
        "        for i in range(sentence_count):\n",
        "            # Calculate new score\n",
        "            new_score = (1 - damping_factor)\n",
        "\n",
        "            sum_of_products = 0\n",
        "            for j in range(sentence_count):\n",
        "                product = similarity_matrix[j][i] * prior_scores[j]\n",
        "                sum_of_products += product\n",
        "\n",
        "\n",
        "            new_score += (damping_factor * sum_of_products)\n",
        "\n",
        "            # Assign new score\n",
        "            scores[i] = new_score\n",
        "        # Check if the change a minute change.\n",
        "        if sum(abs(scores[i] - prior_scores[i]) for i in range(sentence_count)) < epsilon:\n",
        "            break\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "osjToqrM7Xmw"
      },
      "outputs": [],
      "source": [
        "def extract_summary(text, top_n=5):\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Get the sentences from the original text\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Get the similarity matrix from all sentences\n",
        "    sentence_similarity_matrix = construct_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Get the scores for each sentence\n",
        "    scores = text_rank(sentence_similarity_matrix)\n",
        "\n",
        "    # Put the sentences in decsending order based on the score\n",
        "    ranked_sentences = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        ranked_sentences.append((scores[i], s))\n",
        "\n",
        "    ranked_sentences = sorted(ranked_sentences, reverse=True)\n",
        "\n",
        "    # Get the summary sentences and make a combine string.\n",
        "    summary = [sentence for score, sentence in ranked_sentences[:top_n]]\n",
        "    return \" \".join(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK2HhJ3t7jVU"
      },
      "source": [
        "# Confimation Test summarizer works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXtIgGnD7OF3",
        "outputId": "a504c106-2a7c-46a9-e079-ae977f1fd74a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "544\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'numpy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text;\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_tokenize(text)))\n\u001b[1;32m----> 4\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mextract_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
            "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mextract_summary\u001b[1;34m(text, top_n)\u001b[0m\n\u001b[0;32m      6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get the similarity matrix from all sentences\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m sentence_similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_similarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Get the scores for each sentence\u001b[39;00m\n\u001b[0;32m     12\u001b[0m scores \u001b[38;5;241m=\u001b[39m text_rank(sentence_similarity_matrix)\n",
            "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mconstruct_similarity_matrix\u001b[1;34m(all_sentences, stopwords)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_sentences)):\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;66;03m# Calculate similarity score for each sentence pair\u001b[39;00m\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m(i \u001b[38;5;241m!=\u001b[39m j):\n\u001b[1;32m---> 13\u001b[0m     matrix[i][j] \u001b[38;5;241m=\u001b[39m \u001b[43mfind_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Skip same sentence pairs.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[2], line 37\u001b[0m, in \u001b[0;36mfind_similarity\u001b[1;34m(sentence1, sentence2, stopwords)\u001b[0m\n\u001b[0;32m     35\u001b[0m   vector_sentence2[unique_words\u001b[38;5;241m.\u001b[39mindex(word)] \u001b[38;5;241m=\u001b[39mfrequency\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate similarity between vectors\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[43mcosine_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_sentence1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_sentence2\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\cluster\\util.py:130\u001b[0m, in \u001b[0;36mcosine_distance\u001b[1;34m(u, v)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcosine_distance\u001b[39m(u, v):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    Returns 1 minus the cosine of the angle between vectors v and u. This is\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    equal to ``1 - (u.v / |u||v|)``.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[43mnumpy\u001b[49m\u001b[38;5;241m.\u001b[39mdot(u, v) \u001b[38;5;241m/\u001b[39m (sqrt(numpy\u001b[38;5;241m.\u001b[39mdot(u, u)) \u001b[38;5;241m*\u001b[39m sqrt(numpy\u001b[38;5;241m.\u001b[39mdot(v, v))))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'numpy' is not defined"
          ]
        }
      ],
      "source": [
        "text = \"Patient John Doe, a 45-year-old male, presented with a persistent cough, fever, and shortness of breath. The doctor ordered a chest X-ray and blood tests to diagnose the underlying condition. The X-ray revealed signs of pneumonia, and the blood work showed elevated white blood cell count, indicating an infection. The patient was prescribed a course of Azithromycin, an antibiotic, to treat the bacterial pneumonia. Additionally, the doctor recommended taking Ibuprofen to alleviate the fever and body aches associated with the illness. Mr. Doe has a history of hypertension and is currently on Lisinopril to manage his high blood pressure. He also has Type 2 diabetes mellitus and takes Metformin regularly to control his blood sugar levels. During the follow-up visit, the physician noted that the patient's symptoms had improved, and the pneumonia was resolving. However, the doctor advised Mr. Doe to complete the entire course of antibiotics as prescribed to prevent a relapse. Furthermore, the doctor recommended a pulmonary function test to evaluate the patient's lung capacity and rule out any underlying chronic respiratory conditions, such as asthma or chronic obstructive pulmonary disease (COPD). In addition to the medical conditions, the patient reported experiencing occasional heartburn and gastric discomfort. The doctor suggested taking an over-the-counter antacid like Omeprazole to manage the symptoms of acid reflux. Overall, with proper treatment and medication management, the patient's condition is expected to improve, and the risk of complications should be minimized.\"\n",
        "text += text;\n",
        "print(len(word_tokenize(text)))\n",
        "summary = extract_summary(text, top_n=2)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW_NQQqMC2yp"
      },
      "source": [
        "# Methods for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doXE8FrB_XcV"
      },
      "outputs": [],
      "source": [
        "def get_summary_and_original_text(base_path, filename):\n",
        "  # get orginal text\n",
        "  original_path = base_path+\"//business_text/\"+filename\n",
        "  summary_path = base_path+\"/business_summary/\"+filename\n",
        "\n",
        "  original_fhand = open(original_path, 'r')\n",
        "  summary_fhand = open(summary_path, 'r')\n",
        "\n",
        "  original_text = \"\";\n",
        "  summary_text = \"\";\n",
        "\n",
        "  for original_line in original_fhand.readlines():\n",
        "    original_text += original_line.strip()\n",
        "\n",
        "  for summary_line in summary_fhand.readlines():\n",
        "    summary_text += summary_line.strip()\n",
        "\n",
        "  original_fhand.close()\n",
        "  summary_fhand.close()\n",
        "\n",
        "  return (original_text, summary_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2lpHbMeQyRh"
      },
      "outputs": [],
      "source": [
        "def make_summary(original_text, max_word_count):\n",
        "  sentence_count = 1\n",
        "  test_summary = extract_summary(original_text, top_n=sentence_count)\n",
        "\n",
        "  while(len(word_tokenize(test_summary)) < max_word_count):\n",
        "    sentence_count += 1\n",
        "    test_summary = extract_summary(original_text, top_n=sentence_count)\n",
        "\n",
        "  return test_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTCBhk0hRnrR"
      },
      "outputs": [],
      "source": [
        "def get_summary_similarity( original_text, summary_text):\n",
        "  created_summary = word_tokenize(make_summary(original_text, len(word_tokenize(summary_text))))\n",
        "  base_summary_words = word_tokenize(summary_text)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  return  find_similarity(created_summary, base_summary_words,stop_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iptK_wObBQl5",
        "outputId": "4c534086-1f91-42b6-ac31-0a0d26ef72e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:55<00:00,  4.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 0.7351392667558622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Get a list of all files in the folder\n",
        "files_in_folder = os.listdir(folder_path)\n",
        "\n",
        "# Print the names of all files in the folder\n",
        "count = 0\n",
        "sum_score = 0\n",
        "all_files =[[\"file_name\", \"score\"]]\n",
        "\n",
        "for file_name in tqdm(files_in_folder):\n",
        "    original_text, summary_text = get_summary_and_original_text(base_path, file_name)\n",
        "    score = get_summary_similarity(original_text, summary_text)\n",
        "    all_files.append([file_name, score])\n",
        "\n",
        "    count += 1\n",
        "    sum_score += score\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\", (sum_score/count))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyOVp7djtETw"
      },
      "outputs": [],
      "source": [
        "csv_file = \"summary_scores.csv\"\n",
        "csv_fhand = open(csv_file, \"w\", newline=\"\")\n",
        "writer = csv.writer(csv_fhand)\n",
        "writer.writerows(all_files)\n",
        "csv_fhand.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BK2HhJ3t7jVU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
