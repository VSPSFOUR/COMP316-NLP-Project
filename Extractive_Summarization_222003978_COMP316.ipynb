{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Modules\n",
        "  \n",
        "*   Download require nltk components\n",
        "\n"
      ],
      "metadata": {
        "id": "MsE5EZTuAu0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TYAN_4_d91yb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a96e5b8-4d31-4c11-e64e-4f7ffb12a329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding Sentence Similarity\n",
        "\n",
        "\n",
        "*   Cleaning Sentences\n",
        "*   Making vectors from unique words.\n",
        "*   Calculating the consine similarity.\n",
        "\n"
      ],
      "metadata": {
        "id": "2AZtvIQ0ZQV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_similarity(sentence1:list, sentence2:list, stopwords=None):\n",
        "  # Removing all stop words\n",
        "  filtered_sentence1 = [word.lower() for word in sentence1 if word not in stopwords]\n",
        "  filtered_sentence2 = [word.lower() for word in sentence2 if word not in stopwords]\n",
        "\n",
        "\n",
        "  # get unique words\n",
        "  unique_words = list()\n",
        "  unique_words.extend(filtered_sentence1)\n",
        "  unique_words.extend(filtered_sentence2)\n",
        "\n",
        "  unique_words = list(set(unique_words)) # use set to ensure only one case of each word\n",
        "\n",
        "  # Get frequency information\n",
        "  frequency_1 = dict()\n",
        "  frequency_2 = dict()\n",
        "\n",
        "  for word in filtered_sentence1:\n",
        "    frequency_1[word]  = frequency_1.get(word, 0) +1;\n",
        "\n",
        "  for word in filtered_sentence2:\n",
        "    frequency_2[word]  = frequency_2.get(word, 0) +1;\n",
        "\n",
        "  # Create vectors\n",
        "  size_unique_words = len(unique_words)\n",
        "  vector_sentence1 = [0 for i in unique_words]\n",
        "  vector_sentence2 = [0 for i in unique_words]\n",
        "\n",
        "  # Assigning frequencies to vectors\n",
        "  for word,frequency in frequency_1.items():\n",
        "    vector_sentence1[unique_words.index(word)] = frequency\n",
        "\n",
        "  for word,frequency in frequency_2.items():\n",
        "    vector_sentence2[unique_words.index(word)] =frequency\n",
        "  # Calculate similarity between vectors\n",
        "  return 1-cosine_distance(vector_sentence1, vector_sentence2)"
      ],
      "metadata": {
        "id": "DJOGpb5q7S3f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Similarity Matrix\n",
        "* Iterate over all sentences.\n",
        "* Generate  a similarity score for all “sentence pairs” that aren’t the same.\n"
      ],
      "metadata": {
        "id": "sfv_xZLratlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_similarity_matrix(all_sentences, stopwords):\n",
        "  # Initialize matrix\n",
        "  matrix = [\n",
        "\n",
        "            [0 for i in range(len(all_sentences))]\n",
        "            for j in range(len(all_sentences))\n",
        "  ]\n",
        "  # Iterate over all sentences\n",
        "  for i in range(len(all_sentences)):\n",
        "    for j in range(len(all_sentences)):\n",
        "      # Calculate similarity score for each sentence pair\n",
        "      if(i != j):\n",
        "        matrix[i][j] = find_similarity(all_sentences[i], all_sentences[j], stopwords)\n",
        "      else:\n",
        "        # Skip same sentence pairs.\n",
        "        continue\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "nOmxs0lQ7UlB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Text Rank score\n",
        "* Iterate over similarity matrix.\n",
        "* Calculate news score.\n",
        "* Assign score.\n",
        "* Add Stop iteration mechanic.\n"
      ],
      "metadata": {
        "id": "ksTkMC2Ya7jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_rank(similarity_matrix):\n",
        "    # Initialize scores\n",
        "    sentence_count = len(similarity_matrix)\n",
        "    scores = [ 1 for i in range(sentence_count)]\n",
        "    # Set damping factors\n",
        "    damping_factor = 0.85\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    for x in range(100):\n",
        "        # Initialize prior scores\n",
        "        prior_scores = scores.copy()\n",
        "        for i in range(sentence_count):\n",
        "            # Calculate new score\n",
        "            new_score = (1 - damping_factor)\n",
        "\n",
        "            sum_of_products = 0\n",
        "            for j in range(sentence_count):\n",
        "                product = similarity_matrix[j][i] * prior_scores[j]\n",
        "                sum_of_products += product\n",
        "\n",
        "\n",
        "            new_score += (damping_factor * sum_of_products)\n",
        "\n",
        "            # Assign new score\n",
        "            scores[i] = new_score\n",
        "        # Check if the change a minute change.\n",
        "        if sum(abs(scores[i] - prior_scores[i]) for i in range(sentence_count)) < epsilon:\n",
        "            break\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "-R5u11c47WU3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct summary\n",
        "* Tokenize text to sentences.\n",
        "* Apply the above methods.\n",
        "* Get the desired amount of sentences.\n",
        "* Display summary.\n"
      ],
      "metadata": {
        "id": "fYlsx9LobCUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_summary(text, top_n=5):\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Get the sentences from the original text\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Get the similarity matrix from all sentences\n",
        "    sentence_similarity_matrix = construct_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Get the scores for each sentence\n",
        "    scores = text_rank(sentence_similarity_matrix)\n",
        "\n",
        "    # Put the sentences in decsending order based on the score\n",
        "    ranked_sentences = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        ranked_sentences.append((scores[i], s))\n",
        "\n",
        "    ranked_sentences = sorted(ranked_sentences, reverse=True)\n",
        "\n",
        "    # Get the summary sentences and make a combine string.\n",
        "    summary = [sentence for score, sentence in ranked_sentences[:top_n]]\n",
        "    return \" \".join(summary)"
      ],
      "metadata": {
        "id": "osjToqrM7Xmw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Text Rank Algorithm"
      ],
      "metadata": {
        "id": "12TwDAO5dEES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Western Chalukya Empire ruled most of the western Deccan, South India, between the 10th and 12th centuries. This Kannadiga dynasty is sometimes called the Kalyani Chalukya after its regal capital at Kalyani, today's Basavakalyan in the modern Bidar District of Karnataka state, and alternatively the Later Chalukya from its theoretical relationship to the sixth-century Chalukya dynasty of Badami. Prior to the rise of the Western and Eastern Chalukyas, the Rashtrakuta Empire of Manyakheta controlled most of the Deccan and Central India for over two centuries. In 973, seeing confusion in the Rashtrakuta Empire after a successful invasion of their capital by the ruler of the Paramara dynasty of Malwa, Tailapa II, a feudatory of the Rashtrakuta dynasty ruling from Bijapur region, defeated his overlords and made Manyakheta his capital. The dynasty quickly rose to power and grew into an empire under Someshvara I who moved the capital to Kalyani.\"\n",
        "summary = extract_summary(text, top_n=2)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPSxikcM4oUG",
        "outputId": "8bb59549-4c93-42ec-b253-2f137810d7ec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prior to the rise of the Western and Eastern Chalukyas, the Rashtrakuta Empire of Manyakheta controlled most of the Deccan and Central India for over two centuries. The dynasty quickly rose to power and grew into an empire under Someshvara I who moved the capital to Kalyani.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MsE5EZTuAu0V",
        "2AZtvIQ0ZQV8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}