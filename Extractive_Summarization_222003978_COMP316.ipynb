{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VSPSFOUR/COMP316-NLP-Project/blob/Extractive-Summarizer/Extractive_Summarization_222003978_COMP316.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required Modules\n",
        "  \n",
        "*   Download require nltk components\n",
        "\n"
      ],
      "metadata": {
        "id": "MsE5EZTuAu0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYAN_4_d91yb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624e984e-e80c-4aff-aebb-e45dea4a0eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding Sentence Similarity\n",
        "\n",
        "\n",
        "*   Cleaning Sentences\n",
        "*   Making vectors from unique words.\n",
        "*   Calculating the consine similarity.\n",
        "\n"
      ],
      "metadata": {
        "id": "2AZtvIQ0ZQV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_similarity(sentence1:list, sentence2:list, stopwords=None):\n",
        "  # Removing all stop words\n",
        "  filtered_sentence1 = [word.lower() for word in sentence1 if word not in stopwords]\n",
        "  filtered_sentence2 = [word.lower() for word in sentence2 if word not in stopwords]\n",
        "\n",
        "\n",
        "  # get unique words\n",
        "  unique_words = list()\n",
        "  unique_words.extend(filtered_sentence1)\n",
        "  unique_words.extend(filtered_sentence2)\n",
        "\n",
        "  unique_words = list(set(unique_words)) # use set to ensure only one case of each word\n",
        "\n",
        "  # Get frequency information\n",
        "  frequency_1 = dict()\n",
        "  frequency_2 = dict()\n",
        "\n",
        "  for word in filtered_sentence1:\n",
        "    frequency_1[word]  = frequency_1.get(word, 0) +1;\n",
        "\n",
        "  for word in filtered_sentence2:\n",
        "    frequency_2[word]  = frequency_2.get(word, 0) +1;\n",
        "\n",
        "  # Create vectors\n",
        "  size_unique_words = len(unique_words)\n",
        "  vector_sentence1 = [0 for i in unique_words]\n",
        "  vector_sentence2 = [0 for i in unique_words]\n",
        "\n",
        "  # Assigning frequencies to vectors\n",
        "  for word,frequency in frequency_1.items():\n",
        "    vector_sentence1[unique_words.index(word)] = frequency\n",
        "\n",
        "  for word,frequency in frequency_2.items():\n",
        "    vector_sentence2[unique_words.index(word)] =frequency\n",
        "  # Calculate similarity between vectors\n",
        "  return 1-cosine_distance(vector_sentence1, vector_sentence2)"
      ],
      "metadata": {
        "id": "DJOGpb5q7S3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Similarity Matrix\n",
        "* Iterate over all sentences.\n",
        "* Generate  a similarity score for all “sentence pairs” that aren’t the same.\n"
      ],
      "metadata": {
        "id": "sfv_xZLratlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_similarity_matrix(all_sentences, stopwords):\n",
        "  # Initialize matrix\n",
        "  matrix = [\n",
        "\n",
        "            [0 for i in range(len(all_sentences))]\n",
        "            for j in range(len(all_sentences))\n",
        "  ]\n",
        "  # Iterate over all sentences\n",
        "  for i in range(len(all_sentences)):\n",
        "    for j in range(len(all_sentences)):\n",
        "      # Calculate similarity score for each sentence pair\n",
        "      if(i != j):\n",
        "        matrix[i][j] = find_similarity(all_sentences[i], all_sentences[j], stopwords)\n",
        "      else:\n",
        "        # Skip same sentence pairs.\n",
        "        continue\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "nOmxs0lQ7UlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Text Rank score\n",
        "* Iterate over similarity matrix.\n",
        "* Calculate news score.\n",
        "* Assign score.\n",
        "* Add Stop iteration mechanic.\n"
      ],
      "metadata": {
        "id": "ksTkMC2Ya7jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_rank(similarity_matrix):\n",
        "    # Initialize scores\n",
        "    sentence_count = len(similarity_matrix)\n",
        "    scores = [ 1 for i in range(sentence_count)]\n",
        "    # Set damping factors\n",
        "    damping_factor = 0.85\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    for x in range(100):\n",
        "        # Initialize prior scores\n",
        "        prior_scores = scores.copy()\n",
        "        for i in range(sentence_count):\n",
        "            # Calculate new score\n",
        "            new_score = (1 - damping_factor)\n",
        "\n",
        "            sum_of_products = 0\n",
        "            for j in range(sentence_count):\n",
        "                product = similarity_matrix[j][i] * prior_scores[j]\n",
        "                sum_of_products += product\n",
        "\n",
        "\n",
        "            new_score += (damping_factor * sum_of_products)\n",
        "\n",
        "            # Assign new score\n",
        "            scores[i] = new_score\n",
        "        # Check if the change a minute change.\n",
        "        if sum(abs(scores[i] - prior_scores[i]) for i in range(sentence_count)) < epsilon:\n",
        "            break\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "-R5u11c47WU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct summary\n",
        "* Tokenize text to sentences.\n",
        "* Apply the above methods.\n",
        "* Get the desired amount of sentences.\n",
        "* Display summary.\n"
      ],
      "metadata": {
        "id": "fYlsx9LobCUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_summary(text, top_n=5):\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Get the sentences from the original text\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Get the similarity matrix from all sentences\n",
        "    sentence_similarity_matrix = construct_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Get the scores for each sentence\n",
        "    scores = text_rank(sentence_similarity_matrix)\n",
        "\n",
        "    # Put the sentences in decsending order based on the score\n",
        "    ranked_sentences = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        ranked_sentences.append((scores[i], s))\n",
        "\n",
        "    ranked_sentences = sorted(ranked_sentences, reverse=True)\n",
        "\n",
        "    # Get the summary sentences and make a combine string.\n",
        "    summary = [sentence for score, sentence in ranked_sentences[:top_n]]\n",
        "    return \" \".join(summary)"
      ],
      "metadata": {
        "id": "osjToqrM7Xmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Text Rank Algorithm"
      ],
      "metadata": {
        "id": "12TwDAO5dEES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"# Patient John Doe, a 45-year-old male, presented with a persistent cough, fever, and shortness of breath. The doctor ordered a chest X-ray and blood tests to diagnose the underlying condition. The X-ray revealed signs of pneumonia, and the blood work showed elevated white blood cell count, indicating an infection. The patient was prescribed a course of Azithromycin, an antibiotic, to treat the bacterial pneumonia. Additionally, the doctor recommended taking Ibuprofen to alleviate the fever and body aches associated with the illness. Mr. Doe has a history of hypertension and is currently on Lisinopril to manage his high blood pressure. He also has Type 2 diabetes mellitus and takes Metformin regularly to control his blood sugar levels. During the follow-up visit, the physician noted that the patient's symptoms had improved, and the pneumonia was resolving. However, the doctor advised Mr. Doe to complete the entire course of antibiotics as prescribed to prevent a relapse. Furthermore, the doctor recommended a pulmonary function test to evaluate the patient's lung capacity and rule out any underlying chronic respiratory conditions, such as asthma or chronic obstructive pulmonary disease (COPD). In addition to the medical conditions, the patient reported experiencing occasional heartburn and gastric discomfort. The doctor suggested taking an over-the-counter antacid like Omeprazole to manage the symptoms of acid reflux. Overall, with proper treatment and medication management, the patient's condition is expected to improve, and the risk of complications should be minimized.\"\n",
        "\n",
        "text = \"The Western Chalukya Empire ruled most of the western Deccan, South India, between the 10th and 12th centuries. This Kannadiga dynasty is sometimes called the Kalyani Chalukya after its regal capital at Kalyani, today's Basavakalyan in the modern Bidar District of Karnataka state, and alternatively the Later Chalukya from its theoretical relationship to the sixth-century Chalukya dynasty of Badami. Prior to the rise of the Western and Eastern Chalukyas, the Rashtrakuta Empire of Manyakheta controlled most of the Deccan and Central India for over two centuries. In 973, seeing confusion in the Rashtrakuta Empire after a successful invasion of their capital by the ruler of the Paramara dynasty of Malwa, Tailapa II, a feudatory of the Rashtrakuta dynasty ruling from Bijapur region, defeated his overlords and made Manyakheta his capital. The dynasty quickly rose to power and grew into an empire under Someshvara I who moved the capital to Kalyani.\"\n",
        "summary = extract_summary(text, top_n=2)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPSxikcM4oUG",
        "outputId": "fa736e42-ee9b-4f1b-844d-ddb2d13d2bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prior to the rise of the Western and Eastern Chalukyas, the Rashtrakuta Empire of Manyakheta controlled most of the Deccan and Central India for over two centuries. The dynasty quickly rose to power and grew into an empire under Someshvara I who moved the capital to Kalyani.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MsE5EZTuAu0V",
        "2AZtvIQ0ZQV8"
      ],
      "authorship_tag": "ABX9TyM3DZ+dCSn89XUkc3xuq0pE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}